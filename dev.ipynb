{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom\n",
    "from utils import utils, vis\n",
    "from utils import poly_point_isect as bo   ##bentley-ottmann sweep line\n",
    "import criteria as C\n",
    "\n",
    "\n",
    "## third party\n",
    "import networkx as nx\n",
    "\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "\n",
    "\n",
    "## sys\n",
    "import random\n",
    "import time\n",
    "from glob import glob\n",
    "import math\n",
    "\n",
    "## numeric\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## vis\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import collections  as mc\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "## notebook\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = 'cpu'\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-colorblind')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize via Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete old\n",
    "# !rm -r fig\n",
    "!mkdir fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crossings(pos, G, k2i, sampleSize, sampleOn='edges', reg_coef=1, niter=30):\n",
    "#     crossing_segs_sample = utils.sample_crossings(pos, G, k2i, sampleSize, sampleOn)\n",
    "#     if len(crossing_segs_sample) > 0:\n",
    "#         pos_segs = pos[crossing_segs_sample.flatten()].view(-1,4,2)\n",
    "#         w = (torch.rand(pos_segs.shape[0], 2, 1)-0.5).requires_grad_(True)\n",
    "#         b = (torch.rand(pos_segs.shape[0], 1, 1)-0.5).requires_grad_(True)\n",
    "#         relu = nn.ReLU()\n",
    "#         o = optim.SGD([w,b], lr=0.01, momentum=0.5, nesterov=True)\n",
    "#         for _ in range(niter):\n",
    "#             pred = pos_segs.detach() @ w + b\n",
    "#             ## assume labels of nodes in the first edges are -1\n",
    "#             ## now flip the pred of those nodes so that now we want every pred to be +1\n",
    "#             pred[:,:2,:] = -pred[:,:2,:]\n",
    "            \n",
    "#             loss_svm = relu(1-pred).sum() + reg_coef * w.pow(2).sum()\n",
    "#             o.zero_grad()\n",
    "#             loss_svm.backward()\n",
    "#             o.step()\n",
    "#         pred = pos_segs @ w.detach() + b.detach()\n",
    "#         pred[:,:2,:] = -pred[:,:2,:] \n",
    "#         loss_crossing = relu(1-pred).sum()\n",
    "#         return loss_crossing\n",
    "#     else:\n",
    "#         ##return dummy loss\n",
    "#         return (pos[0,0]*0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = nx.Graph()\n",
    "# G.add_nodes_from([0,1,2,3])\n",
    "# G.add_edges_from([(0,1),(2,3)])\n",
    "\n",
    "# print(f'of {len(G)} nodes')\n",
    "# maxDegree = max(dict(G.degree).values())\n",
    "# print('Calculating all pairs shortest path', end='...')\n",
    "\n",
    "# D, k2i = utils.dict2tensor(dict(nx.all_pairs_shortest_path_length(G)))\n",
    "# adj,_ = utils.dict2tensor(dict(G.adjacency()), fill=1, device=device)\n",
    "# i2k = {i:k for k,i in k2i.items()}\n",
    "\n",
    "\n",
    "# W = 1/(D**2+np.eye(len(G)))\n",
    "# truth = adj + torch.eye(adj.shape[0], device=device)\n",
    "# print('done')\n",
    "\n",
    "\n",
    "# ##training\n",
    "# pos = torch.tensor([[-1.0,-1],[1,1],[2,0],[0,1]])\n",
    "# # pos = torch.randn(len(G.nodes), 2, device=device)\n",
    "\n",
    "# pos.requires_grad_(True)\n",
    "\n",
    "# w,b = crossings(pos, G, k2i, reg_coef=2, niter=200, sampleSize=10, )\n",
    "# x,y = torch.meshgrid(torch.linspace(-5,5,20),torch.linspace(-5,5,20))\n",
    "# xy = torch.stack([x.reshape(-1), y.reshape(-1)], -1)\n",
    "# pred = (xy @ w.detach() + b.detach()).view(20,20)\n",
    "\n",
    "# plt.contourf(x,y,pred, levels=np.linspace(-5,5,21), cmap='coolwarm')\n",
    "# plt.contour(x,y,pred, levels=[-1,0,1])\n",
    "\n",
    "# pos_numpy = pos.detach().cpu().numpy()\n",
    "# pos_i = {k: pos_numpy[k2i[k], :2] for k in G.nodes}\n",
    "# nx.draw_networkx(G, pos_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# G = nx.grid_2d_graph(16,32)\n",
    "# D, k2i = utils.dict2tensor(dict(nx.all_pairs_shortest_path_length(G)))\n",
    "# i2k = {i:k for k,i in k2i.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "runtime = []\n",
    "\n",
    "niter = int(1e5)\n",
    "\n",
    "shouldVis = True\n",
    "visIter = 100\n",
    "\n",
    "shouldSnap = True\n",
    "snapIter = 5\n",
    "\n",
    "gClamp = 100\n",
    "minLR = 1e-5\n",
    "maxSampleSize = 128\n",
    "\n",
    "\n",
    "\n",
    "totalTime = 0\n",
    "\n",
    "print('generating graph', end=' ')\n",
    "# G = nx.grid_2d_graph(7,5)\n",
    "# G = nx.hypercube_graph(3)\n",
    "G = nx.balanced_tree(2,4)\n",
    "\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from([0,1,2,3])\n",
    "# G.add_edges_from([(0,1),(2,3)])\n",
    "\n",
    "\n",
    "print(f'of {len(G)} nodes')\n",
    "maxDegree = max(dict(G.degree).values())\n",
    "print('Calculating all pairs shortest path', end='...')\n",
    "t0 = time.time()\n",
    "D, k2i = utils.dict2tensor(dict(nx.all_pairs_shortest_path_length(G)))\n",
    "adj,_ = utils.dict2tensor(dict(G.adjacency()), fill=1, device=device)\n",
    "i2k = {i:k for k,i in k2i.items()}\n",
    "dt = time.time() - t0\n",
    "totalTime += dt\n",
    "\n",
    "W = 1/(D**2+np.eye(len(G)))\n",
    "truth = adj + torch.eye(adj.shape[0], device=device)\n",
    "print('done')\n",
    "\n",
    "\n",
    "##training\n",
    "#     pos = torch.rand(len(G.nodes), 2, device=device)*2-1\n",
    "pos = torch.randn(len(G.nodes), 2, device=device)\n",
    "pos.requires_grad_(True)\n",
    "\n",
    "\n",
    "##LOAD prev layout\n",
    "# G_ = nx.read_gpickle(glob(f'layouts/balanced_tree_{2}_{7}-stress-*.gpickle')[0])\n",
    "# pos = {k2i[k]: G_.nodes[k]['pos'] for k in G_.nodes}\n",
    "# pos = torch.stack([torch.from_numpy(pos[i]) for i in range(len(pos))])\n",
    "# pos = pos.requires_grad_(True)\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD([pos], lr=0.05, momentum=0.5, nesterov=True)\n",
    "optimizer = optim.RMSprop([pos], lr=0.1)\n",
    "patience = np.ceil(np.log2(len(G)))*100\n",
    "# scheduler = None\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.7, patience=patience, \n",
    "                                                 min_lr=minLR, verbose=True)\n",
    "\n",
    "iterBar = tqdm(range(niter))\n",
    "lossCurve = []\n",
    "sampleSize = min(len(G), maxSampleSize)\n",
    "degrees = adj.sum(1).numpy().astype(np.int64)\n",
    "xPath = []\n",
    "\n",
    "for i in iterBar:\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "#     loss = C.stress(pos, D, W, sampleSize)\n",
    "#     loss = C.neighborhood_preseration(pos, G, adj, k2i, i2k, n_roots=2, depth_limit=2)\n",
    "    loss = (\n",
    "        0.01*C.stress(pos, D, W, sampleSize)\n",
    "#         1*C.edge_uniformity(pos, G, k2i, sampleSize-1)\n",
    "        +5 * C.angular_resolution(pos, G, k2i, sampleSize=sampleSize//maxDegree)\n",
    "        # + C.aspect_ratio(pos, sampleSize)\n",
    "#         C.crossing_angle_maximization(pos, G, k2i, i2k, sampleSize=10, sampleOn='crossings') ## slow for large sample size\n",
    "#     +0.01*C.vertex_resolution(pos, sampleSize, target=1/len(G)**0.5)\n",
    "#     + 0.1*C.gabriel(pos, G, k2i, sampleSize=int(sampleSize**0.5))\n",
    "#         +1*C.crossings(pos, G, k2i, reg_coef=0.01, niter=20, sampleSize=4, sampleOn='crossings')\n",
    "#         +0.1\n",
    "#         +C.crossings(pos, G, k2i, reg_coef=0.01, niter=20, sampleSize=20, sampleOn='edges')\n",
    "    )\n",
    "\n",
    "    \n",
    "    if loss.isnan():\n",
    "        print('loss is nan')\n",
    "        break\n",
    "        \n",
    "        \n",
    "    loss.backward()\n",
    "    pos.grad.clamp_(-gClamp, gClamp)\n",
    "    optimizer.step()\n",
    "#     print(pos)\n",
    "    if pos.isnan().any():\n",
    "        print('pos is nan')\n",
    "        break\n",
    "\n",
    "    ##debug info\n",
    "    dt = time.time() - t0\n",
    "    totalTime += dt\n",
    "    if i % int(niter/100) == int(niter/100)-1:\n",
    "        iterBar.set_postfix({\n",
    "            'loss': loss.item(), \n",
    "        })\n",
    "#     print(utils.find_crossings(pos, G.edges, k2i).shape[0])\n",
    "\n",
    "    if len(lossCurve) > 0:\n",
    "        lossCurve.append(0.9*lossCurve[-1] + 0.1*loss.item())\n",
    "    else:\n",
    "        lossCurve.append(loss.item())\n",
    "\n",
    "    if scheduler is not None:\n",
    "#         scheduler.step()\n",
    "        scheduler.step(lossCurve[-1])\n",
    "\n",
    "    if shouldSnap and i % snapIter == 0:\n",
    "        x = pos.detach().cpu().numpy()\n",
    "        xPath.append(x.copy())\n",
    "\n",
    "    ##vis\n",
    "    if shouldVis and i % visIter == visIter-1:\n",
    "        x = pos.detach().cpu().numpy()\n",
    "        pos_i = {k: x[k2i[k], :2] for k in G.nodes}\n",
    "        display.clear_output(wait=True)\n",
    "        vis.plot(G, pos_i, lossCurve, [], i, totalTime, node_size=6, edge=True, show=True, save=False)\n",
    "        prevTime = totalTime\n",
    "        \n",
    "    if optimizer.param_groups[0]['lr'] <= minLR:\n",
    "        print('Done')\n",
    "        break\n",
    "\n",
    "## show final result\n",
    "x = pos.detach().cpu().numpy()   \n",
    "pos_i = {k: x[k2i[k], :2] for k in G.nodes}\n",
    "vis.plot(G, pos_i, lossCurve, [], i, totalTime,  \n",
    "         show=True, save=True, title=f'|V|={len(G)}, iter: {i}, time: {totalTime:.2f} sec')\n",
    "totalTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## without crossing minimization\n",
    "pos = {k2i[k]: G_.nodes[k]['pos'] for k in G_.nodes}\n",
    "vis.plot(G, pos, [], [], 0, 0, node_size=6, edge=True, show=True, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# nx.draw_networkx(G, pos_i, ax=ax)\n",
    "# ax.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.animation import FuncAnimation\n",
    "# from IPython.display import HTML\n",
    "\n",
    "# if type(xPath) == list:\n",
    "#     xPath = np.stack(xPath)\n",
    "    \n",
    "# padding = 0.1\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# scatter = plt.scatter([0], [0], s=10)\n",
    "# lc = mc.LineCollection([], linewidths=1)\n",
    "# ax.add_collection(lc)\n",
    "\n",
    "# xlim = [np.min(xPath[:,:,0]),np.max(xPath[:,:,0])]\n",
    "# ylim = [np.min(xPath[:,:,1]),np.max(xPath[:,:,1])]\n",
    "# xlim = [xlim[0]-padding*(xlim[1]-xlim[0]), xlim[1]+padding*(xlim[1]-xlim[0])]\n",
    "# ylim = [ylim[0]-padding*(ylim[1]-ylim[0]), ylim[1]+padding*(ylim[1]-ylim[0])]\n",
    "# ax.set_xlim(xlim)\n",
    "# ax.set_ylim(ylim)\n",
    "    \n",
    "# def init():\n",
    "#     return scatter,lc\n",
    "\n",
    "# def update(frame):\n",
    "#     xy = xPath[frame]\n",
    "#     scatter.set_offsets(xy)\n",
    "#     segs = [[xy[k2i[k0]], xy[k2i[k1]]] for k0,k1 in G.edges]           \n",
    "#     lc.set_segments(segs)\n",
    "#     return scatter,lc\n",
    "\n",
    "# anim = FuncAnimation(\n",
    "#     fig, \n",
    "#     update, \n",
    "#     frames=range(0,len(xPath),1),\n",
    "#     init_func=init, \n",
    "#     interval=1000.0/20,\n",
    "#     blit=True)\n",
    "\n",
    "# HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = pos.detach().cpu().numpy()\n",
    "# pos_i = {k: x[k2i[k], :2] for k in G.nodes}\n",
    "# vis.plot(G, pos_i, lossHistory, [], i, totalTime, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a gif from images in fig/\n",
    "# # \n",
    "# frames = []\n",
    "# imgs = natsorted(glob('fig/*.png'))\n",
    "\n",
    "# for img in imgs:\n",
    "#     new_frame = Image.open(img)\n",
    "#     frames.append(new_frame)\n",
    "\n",
    "# # Save into a GIF file that loops forever\n",
    "# frames[0].save(f'anim-{int(time.time())}.gif', format='GIF',\n",
    "#                append_images=frames[1:],\n",
    "#                save_all=True,\n",
    "#                duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
