{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "## subclass nx.graph and store adj, D, etc as attribute variables\n",
    "## Grabriel quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom\n",
    "from utils import utils, vis\n",
    "from utils import poly_point_isect as bo   ##bentley-ottmann sweep line\n",
    "\n",
    "import criteria as C\n",
    "import quality as Q\n",
    "import gd2\n",
    "\n",
    "\n",
    "## third party\n",
    "import networkx as nx\n",
    "\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "\n",
    "\n",
    "## sys\n",
    "import random\n",
    "import time\n",
    "from glob import glob\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "## numeric\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io as io\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## vis\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import collections  as mc\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "## notebook\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = 'cpu'\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-colorblind')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_spx_teaser():\n",
    "#     return load_node_link_txt('./graphs/spx_teaser.txt')\n",
    "    \n",
    "    \n",
    "# def load_node_link_txt(fn):\n",
    "    \n",
    "#     def skip_nodes(f,n):\n",
    "#         for _ in range(n):\n",
    "#             f.readline()\n",
    "\n",
    "#     G = nx.Graph()\n",
    "#     with open(fn) as f:\n",
    "#         n = int(f.readline().strip())\n",
    "#         skip_nodes(f, n)\n",
    "#         G.add_nodes_from(range(n))\n",
    "#         for e in f:\n",
    "#             e = e.strip().split()\n",
    "#             e = int(e[0]), int(e[1])\n",
    "#             G.add_edge(*e)\n",
    "#     return G\n",
    "\n",
    "\n",
    "# def load_mat(fn='graphs/SuiteSparse Matrix Collection/grid1_dual.mat'):\n",
    "    \n",
    "#     ## credit:\n",
    "#     ## https://github.com/jxz12/s_gd2/blob/master/jupyter/main.ipynb\n",
    "\n",
    "#     # load the data from the SuiteSparse Matrix Collection format\n",
    "#     # https://www.cise.ufl.edu/research/sparse/matrices/\n",
    "\n",
    "#     mat_data = io.loadmat(fn)\n",
    "#     adj = mat_data['Problem']['A'][0][0]\n",
    "#     G = nx.convert_matrix.from_numpy_matrix(adj.toarray())\n",
    "#     return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize via Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘figures’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# delete old\n",
    "# !rm -r figures\n",
    "!mkdir figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = nx.Graph()\n",
    "# G.add_nodes_from([0,1,2,3])\n",
    "# G.add_edges_from([(0,1),(2,3)])\n",
    "\n",
    "# print(f'of {len(G)} nodes')\n",
    "# maxDegree = max(dict(G.degree).values())\n",
    "# print('Calculating all pairs shortest path', end='...')\n",
    "\n",
    "# D, k2i = utils.dict2tensor(dict(nx.all_pairs_shortest_path_length(G)))\n",
    "# adj,_ = utils.dict2tensor(dict(G.adjacency()), fill=1, device=device)\n",
    "# i2k = {i:k for k,i in k2i.items()}\n",
    "\n",
    "\n",
    "# W = 1/(D**2+np.eye(len(G)))\n",
    "# truth = adj + torch.eye(adj.shape[0], device=device)\n",
    "# print('done')\n",
    "\n",
    "\n",
    "# ##training\n",
    "# pos = torch.tensor([[-1.0,-1],[1,1],[2,0],[0,1]])\n",
    "# # pos = torch.randn(len(G.nodes), 2, device=device)\n",
    "\n",
    "# pos.requires_grad_(True)\n",
    "\n",
    "# w,b = crossings(pos, G, k2i, reg_coef=2, niter=200, sampleSize=10, )\n",
    "# x,y = torch.meshgrid(torch.linspace(-5,5,20),torch.linspace(-5,5,20))\n",
    "# xy = torch.stack([x.reshape(-1), y.reshape(-1)], -1)\n",
    "# pred = (xy @ w.detach() + b.detach()).view(20,20)\n",
    "\n",
    "# plt.contourf(x,y,pred, levels=np.linspace(-5,5,21), cmap='coolwarm')\n",
    "# plt.contour(x,y,pred, levels=[-1,0,1])\n",
    "\n",
    "# pos_numpy = pos.detach().cpu().numpy()\n",
    "# pos_i = {k: pos_numpy[k2i[k], :2] for k in G.nodes}\n",
    "# nx.draw_networkx(G, pos_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d930c2e28374605b3cf75076a7d9a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "import importlib\n",
    "importlib.reload(C)\n",
    "importlib.reload(Q)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(vis)\n",
    "importlib.reload(gd2)\n",
    "\n",
    "\n",
    "\n",
    "# G = nx.grid_2d_graph(6,4)\n",
    "# G = nx.hypercube_graph(3)\n",
    "G = nx.balanced_tree(2,4)\n",
    "# G = utils.load_spx_teaser()\n",
    "# G = nx.karate_club_graph()\n",
    "\n",
    "\n",
    "# graph_name = 'grid1'\n",
    "# graph_name = 'grid1_dual'\n",
    "# graph_name = 'odepa400'\n",
    "# graph_name = 'netz4504_dual'\n",
    "# graph_name = 'L'\n",
    "# -----\n",
    "# graph_name = 'netz4504'\n",
    "# graph_name = 'qh882'\n",
    "# mat_dir = 'graphs/SuiteSparse Matrix Collection'\n",
    "# G = utils.load_mat(f'{mat_dir}/{graph_name}.mat')\n",
    "\n",
    "\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from([0,1,2,3])\n",
    "# G.add_edges_from([(0,1),(2,3)])\n",
    "\n",
    "\n",
    "\n",
    "result = gd2.optimize(\n",
    "    G, niter=50, \n",
    "    sample_sizes={'stress':128, 'neighborhood_preservation': 64},\n",
    "    criteria_weights={'stress':1.0, 'neighborhood_preservation': 0.1}, \n",
    "    evaluate='all',\n",
    "    learning_rate=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-804ae83c3584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qualities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result['qualities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(C)\n",
    "importlib.reload(Q)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(vis)\n",
    "importlib.reload(gd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_G = {k:G.nodes[k]['pos'] for k in G.nodes}\n",
    "vis.plot(\n",
    "    G, pos_G,\n",
    "    result['loss_curve'], \n",
    "    result['iter'], result['runtime'], \n",
    "    edge=True, show=True, save=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tree_depths = [8,]\n",
    "sample_sizes = [4,8,16,32,64,128,256, 512]\n",
    "runs = range(10)\n",
    "\n",
    "df = pd.DataFrame(columns=['tree_depth', 'sample_size', 'runtime'])\n",
    "\n",
    "\n",
    "for tree_depth, sample_size, run_number \\\n",
    "    in tqdm(itertools.product(tree_depths, sample_sizes, runs)):\n",
    "    \n",
    "    G = nx.balanced_tree(2, tree_depth)\n",
    "    result = gd2.optimize(\n",
    "        G, \n",
    "        niter=100000, \n",
    "        sample_size={'stress':sample_size}\n",
    "    )\n",
    "    result['qualities'] = gd2.evaluate(result)\n",
    "\n",
    "    df = df.append({\n",
    "        'run': run_number,\n",
    "        'tree_depth':tree_depth, \n",
    "        'sample_size':sample_size, \n",
    "        'runtime':result['runtime'],\n",
    "        'stress': result['qualities']['stress']\n",
    "    }, ignore_index=True)\n",
    "\n",
    "\n",
    "    pos_G = {k:G.nodes[k]['pos'] for k in G.nodes}\n",
    "    vis.plot(\n",
    "        G, pos_G,\n",
    "        result['loss_curve'], \n",
    "        result['iter'], result['runtime'], \n",
    "        edge=True, show=True, save=False\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./analysis/balanced-tree-2-8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.plot('sample_size', 'runtime', style='o')\n",
    "df.plot('sample_size', 'stress', style='o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    df['runtime'], \n",
    "    df['stress'], \n",
    "    s=df['sample_size']+10, \n",
    "    linewidth=1,\n",
    "    edgecolor='#eee',\n",
    "    alpha=0.8,\n",
    "#     c=df['sample_size']+10,\n",
    "    label='Sample Size',\n",
    ")\n",
    "\n",
    "plt.xscale('symlog')\n",
    "plt.xlabel('(Log) Runtime')\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.ylabel('Stress')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# grid_side_lengths = [(16,32),(16,64),(16,128)]\n",
    "grid_side_lengths = [(16,32)]\n",
    "sample_sizes = [4,8,16,32,64,128,256,512]\n",
    "# sample_sizes = [4,]\n",
    "runs = range(1)\n",
    "\n",
    "\n",
    "\n",
    "for grid_side_length, sample_size, run_number \\\n",
    "    in tqdm(itertools.product(grid_side_lengths, sample_sizes, runs)):\n",
    "    \n",
    "    G = nx.grid_2d_graph(*grid_side_length)\n",
    "    result = gd2.optimize(\n",
    "        G, \n",
    "        niter=100000, \n",
    "        sample_size={'stress':sample_size}\n",
    "    )\n",
    "    result['qualities'] = gd2.evaluate(result)\n",
    "\n",
    "    df = df.append({\n",
    "        'run': run_number,\n",
    "        'type': 'grid_2d_graph',##todo str(G), etc\n",
    "        'grid_side_length': grid_side_length, \n",
    "        'num_nodes': len(G.nodes),\n",
    "        'num_edges': len(G.edges),\n",
    "        'sample_size':sample_size, \n",
    "        'runtime':result['runtime'],\n",
    "        'stress': result['qualities']['stress']\n",
    "    }, ignore_index=True)\n",
    "\n",
    "\n",
    "    pos_G = {k:G.nodes[k]['pos'] for k in G.nodes}\n",
    "    vis.plot(\n",
    "        G, pos_G,\n",
    "        result['loss_curve'], \n",
    "        result['iter'], result['runtime'], \n",
    "        edge=True, show=True, save=False\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./analysis/grid-16-32.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./analysis/sample_size/grid-16-32.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    df['runtime'], \n",
    "    df['stress'], \n",
    "    s=df['sample_size']+10, \n",
    "    linewidth=1,\n",
    "    edgecolor='#eee',\n",
    "    alpha=0.8,\n",
    "#     c=df['sample_size']+10,\n",
    "    label='Sample Size',\n",
    ")\n",
    "\n",
    "plt.xscale('symlog')\n",
    "plt.xlabel('(Log) Runtime')\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.ylabel('Stress')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "import importlib\n",
    "importlib.reload(C)\n",
    "importlib.reload(Q)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(vis)\n",
    "\n",
    "\n",
    "runtime = []\n",
    "\n",
    "niter = int(1e6)\n",
    "\n",
    "shouldVis = True\n",
    "visIter = 5000\n",
    "\n",
    "shouldSnap = True\n",
    "snapIter = 5\n",
    "\n",
    "gClamp = 4\n",
    "minLR = 1e-5\n",
    "maxSampleSize = 64\n",
    "\n",
    "\n",
    "totalTime = 0\n",
    "\n",
    "print('generating graph', end=' ')\n",
    "# G = nx.grid_2d_graph(18,36)\n",
    "# G = nx.hypercube_graph(3)\n",
    "# G = nx.balanced_tree(2,4)\n",
    "G = utils.load_spx_teaser()\n",
    "# G = nx.karate_club_graph()\n",
    "\n",
    "\n",
    "# graph_name = 'grid1'\n",
    "# graph_name = 'grid1_dual'\n",
    "# graph_name = 'odepa400'\n",
    "# graph_name = 'netz4504_dual'\n",
    "# graph_name = 'L'\n",
    "# -----\n",
    "# graph_name = 'netz4504'\n",
    "# graph_name = 'qh882'\n",
    "# mat_dir = 'graphs/SuiteSparse Matrix Collection'\n",
    "# G = utils.load_mat(f'{mat_dir}/{graph_name}.mat')\n",
    "\n",
    "\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from([0,1,2,3])\n",
    "# G.add_edges_from([(0,1),(2,3)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'of {len(G)} nodes')\n",
    "maxDegree = max(dict(G.degree).values())\n",
    "print('Calculating all pairs shortest path', end='...')\n",
    "t0 = time.time()\n",
    "\n",
    "# D, k2i = utils.dict2tensor(dict(nx.all_pairs_shortest_path_length(G)))\n",
    "# adj,_ = utils.dict2tensor(dict(G.adjacency()), fill=1, device=device)\n",
    "D, adj_sparse, k2i = utils.shortest_path(G)\n",
    "adj = torch.from_numpy(adj_sparse.toarray())\n",
    "D = torch.from_numpy(D)\n",
    "\n",
    "i2k = {i:k for k,i in k2i.items()}\n",
    "edge_indices = [(k2i[e0], k2i[e1]) for e0,e1 in G.edges]\n",
    "node_indices = range(len(G))\n",
    "node_index_pairs = np.c_[\n",
    "    np.repeat(node_indices, len(G)),\n",
    "    np.tile(node_indices, len(G))\n",
    "]\n",
    "node_index_pairs_start = 0\n",
    "np.random.shuffle(node_index_pairs)\n",
    "\n",
    "\n",
    "    \n",
    "dt = time.time() - t0\n",
    "totalTime += dt\n",
    "\n",
    "W = 1/(D**2+1e-6)\n",
    "truth = adj + torch.eye(adj.shape[0], device=device)\n",
    "print('done')\n",
    "\n",
    "\n",
    "##training\n",
    "#     pos = torch.rand(len(G.nodes), 2, device=device)*2-1\n",
    "pos = torch.randn(len(G.nodes), 2, device=device)\n",
    "pos.requires_grad_(True)\n",
    "\n",
    "\n",
    "##LOAD prev layout\n",
    "# G_ = nx.read_gpickle(glob(f'layouts/balanced_tree_{2}_{7}-stress-*.gpickle')[0])\n",
    "# pos = {k2i[k]: G_.nodes[k]['pos'] for k in G_.nodes}\n",
    "# pos = torch.stack([torch.from_numpy(pos[i]) for i in range(len(pos))])\n",
    "# pos = pos.requires_grad_(True)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD([pos], lr=0.1, momentum=0.7, nesterov=True)\n",
    "# optimizer = optim.RMSprop([pos], lr=0.01)\n",
    "# optimizer = optim.Adam([pos], lr=0.001)\n",
    "\n",
    "\n",
    "# scheduler = None\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.9, patience=np.ceil(np.log2(len(G)+1))*100, \n",
    "    min_lr=minLR, verbose=True\n",
    ")\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(\n",
    "#     optimizer, \n",
    "#     lr_lambda=lambda e:1*(1-e/niter)\n",
    "# )\n",
    "\n",
    "\n",
    "iterBar = tqdm(range(niter))\n",
    "lossCurve = []\n",
    "sampleSize = min(len(G), maxSampleSize)\n",
    "degrees = adj.sum(1).numpy().astype(np.int64)\n",
    "xPath = []\n",
    "\n",
    "qualityMeasureInterval = max(1, niter // 30)\n",
    "qualityMeasureCurves = defaultdict(list)\n",
    "activeQualityMeasures = [\n",
    "    'stress',\n",
    "#     'edge_uniformity',\n",
    "#     'neighborhood_preservation',\n",
    "#     'crossings',\n",
    "    \n",
    "#     'crossing_angle_maximization',\n",
    "    'aspect_ratio',\n",
    "#     'angular_resolution',\n",
    "#     'vertex_resolution',\n",
    "#     'gabriel',\n",
    "    \n",
    "]\n",
    "\n",
    "for i in iterBar:\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ## optimization\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if node_index_pairs_start >= len(node_index_pairs):\n",
    "        np.random.shuffle(node_index_pairs)\n",
    "        node_index_pairs_start = 0\n",
    "    stress_samples = node_index_pairs[node_index_pairs_start:node_index_pairs_start+sampleSize]\n",
    "    \n",
    "    loss = (\n",
    "        +C.stress(pos, D, W, samples=stress_samples)\n",
    "#         +0.5*C.edge_uniformity(pos, G, k2i, sampleSize-1)\n",
    "#         +0.1*C.angular_resolution(pos, G, k2i, sampleSize=sampleSize//maxDegree)\n",
    "#         + 10*C.aspect_ratio(pos, sampleSize)\n",
    "#         C.crossing_angle_maximization(pos, G, k2i, i2k, sampleSize=10, sampleOn='crossings') ## slow for large sample size\n",
    "#     +0.001*C.vertex_resolution(pos, sampleSize, target=1/len(G)**0.5)\n",
    "#     + 0.1*C.gabriel(pos, G, k2i, sampleSize=int(sampleSize**0.5))\n",
    "#         +1*C.crossings(pos, G, k2i, reg_coef=0.01, niter=20, sampleSize=4, sampleOn='crossings')\n",
    "#         +C.crossings(pos, G, k2i, reg_coef=0.01, niter=20, sampleSize=20, sampleOn='edges')\n",
    "#         C.neighborhood_preseration(pos, G, adj, k2i, i2k, n_roots=2, depth_limit=2)\n",
    "    )\n",
    "    loss.backward()\n",
    "    pos.grad.clamp_(-gClamp, gClamp)\n",
    "    optimizer.step()\n",
    "    \n",
    "    node_index_pairs_start+=sampleSize\n",
    "    \n",
    "    ## debug info\n",
    "    totalTime += time.time() - t0\n",
    "    if loss.isnan():\n",
    "        raise Exception('loss is nan')\n",
    "    if pos.isnan().any():\n",
    "        raise Exception('pos is nan')\n",
    "    \n",
    "    if i % int(niter/100) == int(niter/100)-1:\n",
    "        iterBar.set_postfix({'loss': loss.item(), })    \n",
    "    if len(lossCurve) > 0:\n",
    "        lossCurve.append(0.9*lossCurve[-1] + 0.1*loss.item())\n",
    "    else:\n",
    "        lossCurve.append(loss.item())\n",
    "    if shouldSnap and i % snapIter == 0:\n",
    "        x = pos.detach().cpu().numpy()\n",
    "        xPath.append(x.copy())    \n",
    "    \n",
    "    \n",
    "    ## quality measures\n",
    "    if i % qualityMeasureInterval == qualityMeasureInterval-1:\n",
    "        if 'stress' in activeQualityMeasures:\n",
    "            qualityMeasureCurves['stress'].append(\n",
    "                Q.stress(pos, D, W, None)\n",
    "            )\n",
    "        if 'edge_uniformity' in activeQualityMeasures:\n",
    "            qualityMeasureCurves['edge_uniformity'].append(\n",
    "                Q.edge_uniformity(pos, G, k2i)\n",
    "            )\n",
    "        if 'neighborhood_preservation' in activeQualityMeasures:\n",
    "            qualityMeasureCurves['neighborhood_preservation'].append(\n",
    "                Q.neighborhood_preservation(pos, G, adj, i2k)\n",
    "            )\n",
    "        if 'crossings' in activeQualityMeasures:\n",
    "            qualityMeasureCurves['crossings'].append(\n",
    "                Q.crossings(pos, edge_indices)\n",
    "            )\n",
    "        if 'crossing_angle_maximization' in activeQualityMeasures:\n",
    "            qualityMeasureCurves['crossing_angle_maximization'].append(\n",
    "                Q.crossing_angle_maximization(pos, G.edges, k2i)\n",
    "            )\n",
    "        if 'aspect_ratio' in activeQualityMeasures:\n",
    "            qualityMeasureCurves['aspect_ratio'].append(\n",
    "                Q.aspect_ratio(pos)\n",
    "            )\n",
    "        if 'angular_resolution' in activeQualityMeasures:\n",
    "            qualityMeasureCurves['angular_resolution'].append(\n",
    "                Q.angular_resolution(pos, G, k2i)\n",
    "            )\n",
    "            \n",
    "        if 'vertex_resolution' in activeQualityMeasures:\n",
    "            qualityMeasureCurves['vertex_resolution'].append(\n",
    "                Q.vertex_resolution(pos, target=1/len(G)**0.5)\n",
    "            )\n",
    "        if 'gabriel' in activeQualityMeasures:\n",
    "            qualityMeasureCurves['gabriel'].append(\n",
    "                Q.gabriel(pos, G, k2i)\n",
    "            )\n",
    "        \n",
    "\n",
    "    ##vis\n",
    "    if shouldVis and i % visIter == visIter-1:\n",
    "        x = pos.detach().cpu().numpy()\n",
    "        pos_i = {k: x[k2i[k], :2] for k in G.nodes}\n",
    "        display.clear_output(wait=True)\n",
    "        vis.plot(G, pos_i, lossCurve, [], i, totalTime, node_size=6, edge=True, show=True, save=False)\n",
    "    \n",
    "        \n",
    "    \n",
    "    if scheduler is not None:\n",
    "#         scheduler.step(i)\n",
    "        scheduler.step(lossCurve[-1])\n",
    "    \n",
    "\n",
    "    if optimizer.param_groups[0]['lr'] <= minLR:\n",
    "#     or pos.grad.max() < 1e-3*(pos.max()-pos.min()):\n",
    "        print('Done')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show final result\n",
    "x = pos.detach().cpu().numpy()\n",
    "pos_i = {k: x[k2i[k], :2] for k in G.nodes}\n",
    "vis.plot(G, pos_i, lossCurve, [], i, totalTime,  \n",
    "         show=True, save=False, title=f'|V|={len(G)}, iter: {i}, time: {totalTime:.2f} sec')\n",
    "totalTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for q in activeQualityMeasures:\n",
    "    plt.figure(figsize=[6,3])\n",
    "    plt.plot(qualityMeasureCurves[q],'.-')\n",
    "    plt.ylabel(' '.join(s.capitalize() for s in q.split('_')))\n",
    "    plt.title(' '.join(s.capitalize() for s in q.split('_')))\n",
    "    plt.show()\n",
    "    print(qualityMeasureCurves[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/len(G)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# nx.draw_networkx(G, pos_i, ax=ax)\n",
    "# ax.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.animation import FuncAnimation\n",
    "# from IPython.display import HTML\n",
    "\n",
    "# if type(xPath) == list:\n",
    "#     xPath = np.stack(xPath)\n",
    "    \n",
    "# padding = 0.1\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# scatter = plt.scatter([0], [0], s=10)\n",
    "# lc = mc.LineCollection([], linewidths=1)\n",
    "# ax.add_collection(lc)\n",
    "\n",
    "# xlim = [np.min(xPath[:,:,0]),np.max(xPath[:,:,0])]\n",
    "# ylim = [np.min(xPath[:,:,1]),np.max(xPath[:,:,1])]\n",
    "# xlim = [xlim[0]-padding*(xlim[1]-xlim[0]), xlim[1]+padding*(xlim[1]-xlim[0])]\n",
    "# ylim = [ylim[0]-padding*(ylim[1]-ylim[0]), ylim[1]+padding*(ylim[1]-ylim[0])]\n",
    "# ax.set_xlim(xlim)\n",
    "# ax.set_ylim(ylim)\n",
    "    \n",
    "# def init():\n",
    "#     return scatter,lc\n",
    "\n",
    "# def update(frame):\n",
    "#     xy = xPath[frame]\n",
    "#     scatter.set_offsets(xy)\n",
    "#     segs = [[xy[k2i[k0]], xy[k2i[k1]]] for k0,k1 in G.edges]           \n",
    "#     lc.set_segments(segs)\n",
    "#     return scatter,lc\n",
    "\n",
    "# anim = FuncAnimation(\n",
    "#     fig, \n",
    "#     update, \n",
    "#     frames=range(0,len(xPath),1),\n",
    "#     init_func=init, \n",
    "#     interval=1000.0/20,\n",
    "#     blit=True)\n",
    "\n",
    "# HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = pos.detach().cpu().numpy()\n",
    "# pos_i = {k: x[k2i[k], :2] for k in G.nodes}\n",
    "# vis.plot(G, pos_i, lossHistory, [], i, totalTime, show=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a gif from images in fig/\n",
    "# # \n",
    "# frames = []\n",
    "# imgs = natsorted(glob('fig/*.png'))\n",
    "\n",
    "# for img in imgs:\n",
    "#     new_frame = Image.open(img)\n",
    "#     frames.append(new_frame)\n",
    "\n",
    "# # Save into a GIF file that loops forever\n",
    "# frames[0].save(f'anim-{int(time.time())}.gif', format='GIF',\n",
    "#                append_images=frames[1:],\n",
    "#                save_all=True,\n",
    "#                duration=100, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
